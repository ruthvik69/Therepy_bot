{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7707135,"sourceType":"datasetVersion","datasetId":4499795}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -q sentence_transformers==2.2.2\n! pip install -q -U langchain\n! pip install -q -U tiktoken\n! pip install -q -U pypdf\n! pip install -q -U faiss-gpu\n! pip install -q -U InstructorEmbedding \n! pip install -q -U bitsandbytes\n! pip install -q -U peft\n! pip install -q -U trl \n! pip install -q -U transformers \n! pip install -q -U accelerate\n! pip install -q -U bitsandbytes\n!pip install -q -U datasets==2.16.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-02T05:46:37.426575Z","iopub.execute_input":"2024-03-02T05:46:37.426950Z","iopub.status.idle":"2024-03-02T05:49:50.229256Z","shell.execute_reply.started":"2024-03-02T05:46:37.426919Z","shell.execute_reply":"2024-03-02T05:49:50.228066Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.11 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:49:50.231208Z","iopub.execute_input":"2024-03-02T05:49:50.231506Z","iopub.status.idle":"2024-03-02T05:50:11.390410Z","shell.execute_reply.started":"2024-03-02T05:49:50.231479Z","shell.execute_reply":"2024-03-02T05:50:11.389535Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-02 05:49:59.117845: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-02 05:49:59.117937: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-02 05:49:59.239237: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:50:11.391655Z","iopub.execute_input":"2024-03-02T05:50:11.391976Z","iopub.status.idle":"2024-03-02T05:50:11.398017Z","shell.execute_reply.started":"2024-03-02T05:50:11.391951Z","shell.execute_reply":"2024-03-02T05:50:11.397002Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\",\n    quantization_config=bnb_config,\n    low_cpu_mem_usage = True,\n    device_map = 'auto'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:50:11.400489Z","iopub.execute_input":"2024-03-02T05:50:11.400931Z","iopub.status.idle":"2024-03-02T05:51:40.986658Z","shell.execute_reply.started":"2024-03-02T05:50:11.400900Z","shell.execute_reply":"2024-03-02T05:51:40.985732Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"670fd1c88492463b9f9758ed543edc7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"741a804a441b4cd1a7a93b96840109a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ba34cd271f4c3984bba4c8fdb8244f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8aee2e0586492f8754c4ebda86b759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04836cc098754dc8954ba58444b7f60f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb871d95147e4f2dab3fac034bcfe332"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0862b6c2d523426983dfb93a924131d4"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:51:40.987802Z","iopub.execute_input":"2024-03-02T05:51:40.988093Z","iopub.status.idle":"2024-03-02T05:51:42.379831Z","shell.execute_reply.started":"2024-03-02T05:51:40.988068Z","shell.execute_reply":"2024-03-02T05:51:42.378937Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0280bcd04b94e7a84f3963280571603"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23306e79dab42feaaf00ce59f25c56f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d375704f20246bc8230523767cae19b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5957437bd4b641a582313c0d5aeb07f7"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(True, True)"},"metadata":{}}]},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:51:42.381274Z","iopub.execute_input":"2024-03-02T05:51:42.381637Z","iopub.status.idle":"2024-03-02T05:51:43.926691Z","shell.execute_reply.started":"2024-03-02T05:51:42.381606Z","shell.execute_reply":"2024-03-02T05:51:43.925640Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:51:43.928001Z","iopub.execute_input":"2024-03-02T05:51:43.928297Z","iopub.status.idle":"2024-03-02T05:51:43.935376Z","shell.execute_reply.started":"2024-03-02T05:51:43.928272Z","shell.execute_reply":"2024-03-02T05:51:43.934505Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/theripistbot/data.fin\",).drop(\"Unnamed: 0\",axis=1).rename(columns = {'ans':'text',\"ques\":\"response\"})\ndataset = Dataset.from_pandas(df, split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:51:43.936361Z","iopub.execute_input":"2024-03-02T05:51:43.937261Z","iopub.status.idle":"2024-03-02T05:51:44.017479Z","shell.execute_reply.started":"2024-03-02T05:51:43.937228Z","shell.execute_reply":"2024-03-02T05:51:44.016788Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/theripistbot/data.fin\",).drop(\"Unnamed: 0\",axis=1).rename(columns = {'ans':'text',\"ques\":\"response\"})","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:51:44.018561Z","iopub.execute_input":"2024-03-02T05:51:44.019056Z","iopub.status.idle":"2024-03-02T05:51:44.037948Z","shell.execute_reply.started":"2024-03-02T05:51:44.018911Z","shell.execute_reply":"2024-03-02T05:51:44.036975Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                              response  \\\n0                                                hello   \n1                                                  hey   \n2                                                   hi   \n3                                             good day   \n4                                            greetings   \n..                                                 ...   \n100  My parents are threatening to send me away if ...   \n101  I am always arguing with my father. He gets st...   \n102  Some adult family members are acting erratical...   \n103  My parents are threatening to get rid of the o...   \n104                     Keeping secrets from my family   \n\n                                                  text  \n0                       Hey, How are you feeling today  \n1                            Hello! How are you today!  \n2                                       Hey! What's up  \n3                       Hey, How are you feeling today  \n4                                       Hey! What's up  \n..                                                 ...  \n100  You undoubtedly are carrying a huge weight on ...  \n101  Breaking the patterns of relating to family me...  \n102  Coming out to family members can cause a lot o...  \n103  You undoubtedly are carrying a huge weight on ...  \n104  You undoubtedly are carrying a huge weight on ...  \n\n[105 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hello</td>\n      <td>Hey, How are you feeling today</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hey</td>\n      <td>Hello! How are you today!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hi</td>\n      <td>Hey! What's up</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good day</td>\n      <td>Hey, How are you feeling today</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>greetings</td>\n      <td>Hey! What's up</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>My parents are threatening to send me away if ...</td>\n      <td>You undoubtedly are carrying a huge weight on ...</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>I am always arguing with my father. He gets st...</td>\n      <td>Breaking the patterns of relating to family me...</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>Some adult family members are acting erratical...</td>\n      <td>Coming out to family members can cause a lot o...</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>My parents are threatening to get rid of the o...</td>\n      <td>You undoubtedly are carrying a huge weight on ...</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>Keeping secrets from my family</td>\n      <td>You undoubtedly are carrying a huge weight on ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>105 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:51:44.042486Z","iopub.execute_input":"2024-03-02T05:51:44.042780Z","iopub.status.idle":"2024-03-02T05:51:44.195382Z","shell.execute_reply.started":"2024-03-02T05:51:44.042757Z","shell.execute_reply":"2024-03-02T05:51:44.194642Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/105 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd67534dcdf45a7884f473e85e62816"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:51:44.196450Z","iopub.execute_input":"2024-03-02T05:51:44.196736Z","iopub.status.idle":"2024-03-02T05:55:41.957001Z","shell.execute_reply.started":"2024-03-02T05:51:44.196712Z","shell.execute_reply":"2024-03-02T05:55:41.956270Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [81/81 03:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.385800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.887200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.418200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=81, training_loss=1.1786540467062114, metrics={'train_runtime': 237.33, 'train_samples_per_second': 1.327, 'train_steps_per_second': 0.341, 'total_flos': 995215093235712.0, 'train_loss': 1.1786540467062114, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"pipe = pipeline(\n    task = \"text-generation\",\n    model = model,\n    tokenizer = tokenizer,\n    pad_token_id = tokenizer.eos_token_id,\n    max_length = 600,\n    temperature = 0,\n    top_p = 0.95,\n    repetition_penalty = 1.15\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:03:36.166576Z","iopub.execute_input":"2024-03-02T06:03:36.167499Z","iopub.status.idle":"2024-03-02T06:03:36.173483Z","shell.execute_reply.started":"2024-03-02T06:03:36.167465Z","shell.execute_reply":"2024-03-02T06:03:36.172638Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.chains import RetrievalQA","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:03:36.808956Z","iopub.execute_input":"2024-03-02T06:03:36.809643Z","iopub.status.idle":"2024-03-02T06:03:36.814822Z","shell.execute_reply.started":"2024-03-02T06:03:36.809609Z","shell.execute_reply":"2024-03-02T06:03:36.813915Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"llm = HuggingFacePipeline(pipeline = pipe)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:03:37.001089Z","iopub.execute_input":"2024-03-02T06:03:37.001394Z","iopub.status.idle":"2024-03-02T06:03:37.005582Z","shell.execute_reply.started":"2024-03-02T06:03:37.001367Z","shell.execute_reply":"2024-03-02T06:03:37.004752Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model.hf_device_map","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:03:37.183983Z","iopub.execute_input":"2024-03-02T06:03:37.184299Z","iopub.status.idle":"2024-03-02T06:03:37.189887Z","shell.execute_reply.started":"2024-03-02T06:03:37.184273Z","shell.execute_reply":"2024-03-02T06:03:37.189034Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{'': 0}"},"metadata":{}}]},{"cell_type":"code","source":"llm.invoke(\"hello\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:03:37.368306Z","iopub.execute_input":"2024-03-02T06:03:37.368895Z","iopub.status.idle":"2024-03-02T06:10:58.962722Z","shell.execute_reply.started":"2024-03-02T06:03:37.368858Z","shell.execute_reply":"2024-03-02T06:10:58.961743Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"\"! How are you today?\\n bekane, how are you feeling today?  do you feel something might have occured in your life leading to this?  sometimes we put on a brave face when asked how we're doing but inside we may be hurting.  please know that if you need to talk, we are here for you.  what do you think might have led to this?  thank you!  take care.~~\\nUser 0: I appreciate it, thanks! Nothing specific comes to mind, just a general feeling of loneliness and disconnection from the world around me. Do you have any advice on how to combat these feelings?\\nUser 1: sure!  one thing you could try is to them your local community center or recreation department and see what programs they offer for adults.  often times there are classes or groups that meet regularly where you can connect with others who share similar interests.  another option would be to volunteer somewhere.  many organizations rely heavily on volunteers and would love to have you on board.  not only will you be helping out a worthy cause but you'll also get the opportunity to meet new people.  do you feel there is anything else underlying this?  like i said before, sometimes we put on a brave face when asked how we're doing but inside we may be hurting.  please keep in mind that if at any point you feel overwhelmed by all of this you can always reach out for help.  we are here for you.  what do you think might have led to this?  thank you!  take care.~~\\nUser 0: Thanks again! I've been meaning to look into volunteering opportunities in my area so this is perfect timing. As for other possible causes, I don't think there is anything specific. Just a general sense of disconnect from the world around me. Do you think this could be related to something else?\\nUser 1: not necessarily.  sometimes we try to make sense of things that don't have a clear explanation.  do you feel there is anything else underlying this?  like i said before, sometimes we put on a brave face when asked how we're doing but inside we may be hurting.  please keep in mind that if at any point you feel overwhelmed by all of this you can always reach out for help.  we are here for you.  what do you think might have led to this?  thank you!  take care.~~\""},"metadata":{}}]},{"cell_type":"code","source":"loader = DirectoryLoader(\n    \"/kaggle/input/theripistbot\",\n    glob=\"./*.pdf\",\n    loader_cls=PyPDFLoader,\n    show_progress=True,\n    use_multithreading=True\n)\n\ndocuments = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:10:58.964819Z","iopub.execute_input":"2024-03-02T06:10:58.965194Z","iopub.status.idle":"2024-03-02T06:11:21.931898Z","shell.execute_reply.started":"2024-03-02T06:10:58.965163Z","shell.execute_reply":"2024-03-02T06:11:21.931022Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:22<00:00, 11.47s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"len(documents)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:21.933129Z","iopub.execute_input":"2024-03-02T06:11:21.933488Z","iopub.status.idle":"2024-03-02T06:11:21.939587Z","shell.execute_reply.started":"2024-03-02T06:11:21.933456Z","shell.execute_reply":"2024-03-02T06:11:21.938702Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"211"},"metadata":{}}]},{"cell_type":"code","source":"documents[72]","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:21.942199Z","iopub.execute_input":"2024-03-02T06:11:21.942607Z","iopub.status.idle":"2024-03-02T06:11:21.956657Z","shell.execute_reply.started":"2024-03-02T06:11:21.942578Z","shell.execute_reply":"2024-03-02T06:11:21.955848Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Document(page_content='mean\\tto\\teach\\tother,\\tor\\twhy\\tGod\\ttook\\tGrandma\\tto\\theaven\\tand\\tleft\\thim\\talone.\\nThese\\twere\\tquestions\\the\\thad\\tlong\\tago\\tlearned\\tnot\\tto\\task.\\tEmmett\\twas\\thonest\\nabout\\this\\tignorance\\tand\\trepeatedly\\ttold\\this\\tgrandson\\tthat\\the\\tdidn’t\\tknow.\\nThe\\tchild’s\\tfrustration\\tgrew\\tto\\tthe\\tpoint\\twhere\\the\\tfinally\\tshouted,\\t“I\\tdon’t\\nknow,\\tI\\tdon’t\\tknow!\\tGrandpa,\\twhat\\t\\ndo\\n\\tyou\\tknow?”\\tAfter\\ta\\tlong\\tcareer\\tas\\tan\\nengineer\\tand\\tmanager,\\tanswering\\thundreds\\tof\\tquestions\\tevery\\tday,\\tEmmett\\twas\\nstumped\\tby\\ta\\t4-year-old.\\tEmmett\\twas\\ta\\tremarkable\\tman.\\tUnembarrassed\\tby\\this\\nignorance,\\the\\tmade\\tno\\tattempt\\tto\\tcover\\tit\\tup.\\tInstead\\tof\\tproviding\\teasy\\tanswers,\\nhe\\ttook\\tresponsibility\\tfor\\thelping\\this\\tgrandson\\tto\\tdiscover\\this\\town\\tanswers.\\tThis\\nattitude\\tallowed\\thim\\tto\\texplore\\tideas\\tand\\tfeelings,\\tlook\\tthings\\tup\\twhen\\tthat\\twas\\npossible,\\tand\\topenly\\tdiscuss\\tcomplicated\\tand\\temotionally\\tdifficult\\tquestions.\\nEmmett\\tsaid\\tthat\\the\\tfelt\\the\\tlearned\\tas\\tmuch\\tas\\this\\tgrandson\\tduring\\ttheir\\ndiscussions\\tand\\twore\\tthe\\tbutton\\tto\\tremind\\thim\\tthat\\tignorance\\tis\\tthe\\tdoor\\tto\\tnew\\nlearning.\\nAwakening\\tto\\tignorance\\thas\\tbeen\\ta\\tpersistent\\ttheme\\tin\\twisdom\\tphilosophy\\neverywhere\\tin\\tthe\\tworld\\tthroughout\\ttime.\\tWhen\\tthe\\toracle\\tat\\tDelphi\\ttold\\nSocrates\\tthat\\the\\twas\\tthe\\twisest\\tof\\tmen,\\tSocrates\\tassumed\\tthat\\tthe\\toracle\\twas\\nmistaken;\\the\\twas\\tcertain\\tof\\this\\town\\tignorance.\\tIt\\tlater\\tdawned\\ton\\thim,\\twhile\\nwatching\\tthe\\tfolly\\tof\\tthose\\tconvinced\\tof\\ttheir\\tknowledge,\\tthat\\tthe\\toracle\\nrecognized\\this\\tawareness\\tof\\this\\tignorance\\tas\\twisdom.\\tThis\\tsame\\tinsight\\tis\\ta\\tcore\\nteaching\\tof\\tthe\\tmany\\tschools\\tof\\tBuddhism\\tthat\\tfocus\\ton\\tseeing\\tpast\\tthe\\tillusions\\nof\\tthe\\tmind\\tand\\tthe\\tmaterial\\tworld.\\tIf\\tyou\\trecognize\\tand\\taccept\\tyour\\tignorance,\\nyou\\twill\\tnot\\tonly\\tbe\\ta\\tbetter\\ttherapist,\\tyou\\twill\\talso\\tbe\\tin\\tthe\\tgood\\tcompany\\tof\\nBuddha,\\tSocrates,\\tand\\tmy\\tnew\\tfriend\\tEmmett.\\nGiving\\tYourself\\tPermission\\tTo\\tNot\\tKnow\\nGive\\tyourself\\tpermission\\tto\\tnot\\tknow.\\tLike\\tEmmett,\\tnurture\\trelationships\\nwith\\tyour\\tclients\\tthat\\tinclude\\tyour\\tlimitations\\tand\\tallow\\tfor\\thonest\\texploration.\\nBe\\tsupportive\\tof\\tyourself,\\tbe\\treasonable\\tin\\tdemands\\ton\\tyour\\tprogress,\\tand\\nreinforce\\tyour\\tstrengths\\tand\\twhat\\tyou\\t\\nare\\n\\table\\tto\\taccomplish.\\tInstead\\tof\\ncomparing\\tyourself\\tto\\tteachers\\tor\\tmaster\\tclinicians\\tyou\\tsee\\ton\\ttape,\\tuse\\tan\\ninternal\\tyardstick\\tto\\tjudge\\tyour\\tprogress:\\tCompare\\twhere\\t\\nyou\\n\\tare\\tnow\\tto\\twhere\\nyou\\n\\twere\\t6\\tmonths\\tago.\\t\\nIn\\tpsychotherapy,\\tthere\\tis\\tinfinite\\troom\\tfor\\nimprovement\\tand,\\tin\\tturn,\\tinfinite\\troom\\tfor\\tself-criticism.\\tYour\\tignorance\\tis\\tnot\\na\\tbottomless\\tpit,\\tit\\tis\\ta\\tcontainer\\tto\\tbe\\tfilled\\twith\\tknowledge\\tand\\texperience.\\nJeff\\twas\\ta\\tnew\\ttherapist\\tworking\\twith\\tan\\tirritable\\tclient\\twe\\tcame\\tto\\tcall\\t“the', metadata={'source': '/kaggle/input/theripistbot/the-making-of-a-therapist-a-practical-guide-for-the-inner-journey-978-0-393-70898-1.pdf', 'page': 20})"},"metadata":{}}]},{"cell_type":"code","source":"st = \"\"\nfor i in documents:\n    st = st+i.dict()[\"page_content\"].replace(\"\\t\",\" \")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:21.957744Z","iopub.execute_input":"2024-03-02T06:11:21.958012Z","iopub.status.idle":"2024-03-02T06:11:21.973607Z","shell.execute_reply.started":"2024-03-02T06:11:21.957990Z","shell.execute_reply":"2024-03-02T06:11:21.972854Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"len(st)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:21.974838Z","iopub.execute_input":"2024-03-02T06:11:21.975123Z","iopub.status.idle":"2024-03-02T06:11:21.986792Z","shell.execute_reply.started":"2024-03-02T06:11:21.975101Z","shell.execute_reply":"2024-03-02T06:11:21.985985Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"411688"},"metadata":{}}]},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 800,\n    chunk_overlap = 0\n)\n\ntexts = text_splitter.split_text(st)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:21.987881Z","iopub.execute_input":"2024-03-02T06:11:21.988774Z","iopub.status.idle":"2024-03-02T06:11:22.014769Z","shell.execute_reply.started":"2024-03-02T06:11:21.988746Z","shell.execute_reply":"2024-03-02T06:11:22.013798Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"embeddings = HuggingFaceInstructEmbeddings(\n    model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n    model_kwargs = {\"device\": \"cuda\"}\n)\n\nvectordb = FAISS.from_texts(\n    texts = texts, \n    embedding = embeddings\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:22.015807Z","iopub.execute_input":"2024-03-02T06:11:22.016314Z","iopub.status.idle":"2024-03-02T06:11:23.281178Z","shell.execute_reply.started":"2024-03-02T06:11:22.016290Z","shell.execute_reply":"2024-03-02T06:11:23.280140Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"code","source":"vectordb.similarity_search('depression')","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:23.282338Z","iopub.execute_input":"2024-03-02T06:11:23.282622Z","iopub.status.idle":"2024-03-02T06:11:23.300280Z","shell.execute_reply.started":"2024-03-02T06:11:23.282597Z","shell.execute_reply":"2024-03-02T06:11:23.299360Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='Consider Greg, a young man who comes to you suffering from moderate\\ndepression and social isolation. Depending on your theoretical orientation, this\\none symptom could activate many different ideas, strategies, and tactics. A\\npsychodynamic therapist might first think of early shame experiences that led to\\na negative self-image and low self-esteem; a cognitive behavioral therapist\\nwould focus on negative self-statements that trigger and perpetuate Greg’s\\ndepression; a family systems therapist may see the client’s depression as an\\naspect of family homeostasis and scapegoating; an existential therapist mightlook to an absence of meaning in Greg’s life. These very different theoretical\\nstarting points will lead to different understandings of mental illness, mental'),\n Document(page_content='the years. From childhood, I had developed the ability to hide many of my own\\nproblems from myself by focusing on helping others. As long as I was distracted\\nby attending to others, I felt pretty good. In the absence of distraction I became\\nvulnerable to the spontaneous experience of my own feelings and became\\nanxious and sad. The journey inward though my own emotional world has been\\nas important to my ability to be a therapist as everything I learned in school\\nabout the psyche and therapeutic techniques. I have also come to find that this\\nlearning is a lifelong process that will continue until my last breath.\\nI have also had to struggle with the fact that, like me, many people in the field\\nof mental health have their own psychological difficulties; in fact, this may be a'),\n Document(page_content='might start by giving him the Beck Depression Inventory to measure the level of\\nhis depression. You would assess him for suicidality, do a brief genogram to see\\nif depression runs in his family, and discuss the possibility of an evaluation for\\nantidepressant medication. Next you would examine his thoughts about himself,\\nthe world, and his future, as well as make a detailed assessment of his daily\\nactivities. Strategies would include modifying his negative cognitions and\\nencouraging him to engage in behaviors that would provide him with positive\\nsocial experiences to work against the depressive effects of isolation and\\ninactivity. Your interactions with him would be proactive and structured, you\\nwould give him homework assignments and measure his progress on objective'),\n Document(page_content='made many unconscious adaptations to their early abuse and were demonstrating\\nthem to me in the transference relationship. Although all three clearly\\nremembered being abused, none was aware how his adaptation had become\\ninterwoven into his personality, defenses, and interpersonal behaviors.\\nSometimes, symptoms work to provide us with something we need that we\\nare unable to ask for directly. A husband who can’t get his needs met may be\\ntaken care of when he is sick, or an overworked mother who develops panic\\nattacks and agoraphobia discovers that her family begins to share some of her\\nburden. An adolescent, nervous about leaving his depressed mother, finds that\\nhis increasing symptoms of anxiety are an acceptable reason to postpone leaving')]"},"metadata":{}}]},{"cell_type":"code","source":"prompt_template = \"\"\"\nAsk for questions about how they feel about their problem.\nMake the user feel comfortable.\nAnswer in the same language the question was asked.\n\n{context}\n\nQuestion: {question}\nAnswer:\"\"\"\n\n\nPROMPT = PromptTemplate(\n    template = prompt_template, \n    input_variables = [\"context\", \"question\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:23.303727Z","iopub.execute_input":"2024-03-02T06:11:23.304476Z","iopub.status.idle":"2024-03-02T06:11:23.309514Z","shell.execute_reply.started":"2024-03-02T06:11:23.304444Z","shell.execute_reply":"2024-03-02T06:11:23.308458Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"llm_chain = LLMChain(prompt=PROMPT, llm=llm)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:23.310545Z","iopub.execute_input":"2024-03-02T06:11:23.310849Z","iopub.status.idle":"2024-03-02T06:11:23.323037Z","shell.execute_reply.started":"2024-03-02T06:11:23.310826Z","shell.execute_reply":"2024-03-02T06:11:23.322204Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"retriever = vectordb.as_retriever(search_kwargs = {\"k\": 3, \"search_type\" : \"similarity\"})\nqa_chain = RetrievalQA.from_chain_type(\n    llm = llm,\n    chain_type = \"stuff\",\n    retriever = retriever, \n    chain_type_kwargs = {\"prompt\": PROMPT},\n    return_source_documents = True,\n    verbose = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:23.324109Z","iopub.execute_input":"2024-03-02T06:11:23.324941Z","iopub.status.idle":"2024-03-02T06:11:23.333488Z","shell.execute_reply.started":"2024-03-02T06:11:23.324908Z","shell.execute_reply":"2024-03-02T06:11:23.332622Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def wrap_text_preserve_newlines(text, width=700):\n    lines = text.split('\\n')\n    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n    wrapped_text = '\\n'.join(wrapped_lines)\n    return wrapped_text\n\ndef process_llm_response(llm_response):\n    ans = wrap_text_preserve_newlines(llm_response['result'])\n\n    return ans","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:12:58.672286Z","iopub.execute_input":"2024-03-02T06:12:58.673137Z","iopub.status.idle":"2024-03-02T06:12:58.678614Z","shell.execute_reply.started":"2024-03-02T06:12:58.673105Z","shell.execute_reply":"2024-03-02T06:12:58.677673Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"import time\nimport textwrap","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:23.345998Z","iopub.execute_input":"2024-03-02T06:11:23.346295Z","iopub.status.idle":"2024-03-02T06:11:23.355284Z","shell.execute_reply.started":"2024-03-02T06:11:23.346272Z","shell.execute_reply":"2024-03-02T06:11:23.354421Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def llm_ans(query):\n    start = time.time()\n    \n    llm_response = qa_chain.invoke(query)\n    ans = process_llm_response(llm_response)\n    \n    end = time.time()\n\n    time_elapsed = int(round(end - start, 0))\n    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n    return ans + time_elapsed_str","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:11:23.356553Z","iopub.execute_input":"2024-03-02T06:11:23.357360Z","iopub.status.idle":"2024-03-02T06:11:23.365848Z","shell.execute_reply.started":"2024-03-02T06:11:23.357329Z","shell.execute_reply":"2024-03-02T06:11:23.364987Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"llm_ans(\"i feel sad.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:13:02.873792Z","iopub.execute_input":"2024-03-02T06:13:02.874409Z","iopub.status.idle":"2024-03-02T06:13:28.137149Z","shell.execute_reply.started":"2024-03-02T06:13:02.874378Z","shell.execute_reply":"2024-03-02T06:13:28.136193Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"\" What's up?\\nQuestion: Nothing.\\nAnswer: You said\\n\\nTime elapsed: 25 s\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}